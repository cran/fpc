
R version 3.4.3 (2017-11-30) -- "Kite-Eating Tree"
Copyright (C) 2017 The R Foundation for Statistical Computing
Platform: x86_64-pc-linux-gnu (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

  Natural language support but running in an English locale

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> pkgname <- "fpc"
> source(file.path(R.home("share"), "R", "examples-header.R"))
> options(warn = 1)
> library('fpc')
> 
> base::assign(".oldSearch", base::search(), pos = 'CheckExEnv')
> cleanEx()
> nameEx("adcoord")
> ### * adcoord
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: adcoord
> ### Title: Asymmetric discriminant coordinates
> ### Aliases: adcoord
> ### Keywords: multivariate classif
> 
> ### ** Examples
> 
>   set.seed(4634)
>   face <- rFace(600,dMoNo=2,dNoEy=0)
>   grface <- as.integer(attr(face,"grouping"))
>   adcf <- adcoord(face,grface==2)
>   adcf2 <- adcoord(face,grface==4)
>   plot(adcf$proj,col=1+(grface==2))
>   plot(adcf2$proj,col=1+(grface==4))
>   # ...done in one step by function plotcluster.
> 
> 
> 
> cleanEx()
> nameEx("ancoord")
> ### * ancoord
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: ancoord
> ### Title: Asymmetric neighborhood based discriminant coordinates
> ### Aliases: ancoord
> ### Keywords: multivariate classif
> 
> ### ** Examples
> 
>   set.seed(4634)
>   face <- rFace(600,dMoNo=2,dNoEy=0)
>   grface <- as.integer(attr(face,"grouping"))
>   ancf2 <- ancoord(face,grface==4)
>   plot(ancf2$proj,col=1+(grface==4))
>   # ...done in one step by function plotcluster.
> 
> 
> 
> cleanEx()
> nameEx("awcoord")
> ### * awcoord
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: awcoord
> ### Title: Asymmetric weighted discriminant coordinates
> ### Aliases: awcoord
> ### Keywords: multivariate classif
> 
> ### ** Examples
> 
>   set.seed(4634)
>   face <- rFace(600,dMoNo=2,dNoEy=0)
>   grface <- as.integer(attr(face,"grouping"))
>   awcf <- awcoord(face,grface==1)
>   # awcf2 <- ancoord(face,grface==1, method="mcd")
>   plot(awcf$proj,col=1+(grface==1))
>   # plot(awcf2$proj,col=1+(grface==1))
>   # ...done in one step by function plotcluster.
> 
> 
> 
> cleanEx()
> nameEx("batcoord")
> ### * batcoord
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: batcoord
> ### Title: Bhattacharyya discriminant projection
> ### Aliases: batcoord batvarcoord
> ### Keywords: multivariate classif
> 
> ### ** Examples
> 
> set.seed(4634)
> face <- rFace(600,dMoNo=2,dNoEy=0)
> grface <- as.integer(attr(face,"grouping"))
> bcf2 <- batcoord(face,grface==2)
> plot(bcf2$proj,col=1+(grface==2))
> bcfv2 <- batcoord(face,grface==2,dom="variance")
> plot(bcfv2$proj,col=1+(grface==2))
> bcfvv2 <- batvarcoord(face,grface==2)
> plot(bcfvv2$proj,col=1+(grface==2))
> 
> 
> 
> cleanEx()
> nameEx("bhattacharyya.dist")
> ### * bhattacharyya.dist
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: bhattacharyya.dist
> ### Title: Bhattacharyya distance between Gaussian distributions
> ### Aliases: bhattacharyya.dist
> ### Keywords: multivariate
> 
> ### ** Examples
> 
>   round(bhattacharyya.dist(c(1,1),c(2,5),diag(2),diag(2)),digits=2)
modulus 
   2.12 
> 
> 
> 
> cleanEx()
> nameEx("bhattacharyya.matrix")
> ### * bhattacharyya.matrix
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: bhattacharyya.matrix
> ### Title: Matrix of pairwise Bhattacharyya distances
> ### Aliases: bhattacharyya.matrix
> ### Keywords: cluster multivariate
> 
> ### ** Examples
> 
>   muarray <- cbind(c(0,0),c(0,0.1),c(10,10))
>   sigmaarray <- array(c(diag(2),diag(2),diag(2)),dim=c(2,2,3))
>   bhattacharyya.matrix(muarray,sigmaarray,ipairs=list(c(1,2),c(2,3)))
          [,1]         [,2]        [,3]
[1,]        NA 9.987508e-01          NA
[2,] 0.9987508           NA 1.78102e-11
[3,]        NA 1.781020e-11          NA
> 
> 
> 
> 
> cleanEx()
> nameEx("calinhara")
> ### * calinhara
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: calinhara
> ### Title: Calinski-Harabasz index
> ### Aliases: calinhara
> ### Keywords: cluster
> 
> ### ** Examples
> 
>   set.seed(98765)
>   iriss <- iris[sample(150,20),-5]
>   km <- kmeans(iriss,3)
>   round(calinhara(iriss,km$cluster),digits=2)
[1] 63.34
> 
> 
> 
> cleanEx()
> nameEx("can")
> ### * can
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: can
> ### Title: Generation of the tuning constant for regression fixed point
> ###   clusters
> ### Aliases: can
> ### Keywords: arith
> 
> ### ** Examples
> 
>   can(429,3)
[1] 8.806634
> 
> 
> 
> cleanEx()
> nameEx("cat2bin")
> ### * cat2bin
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: cat2bin
> ### Title: Recode nominal variables to binary variables
> ### Aliases: cat2bin
> ### Keywords: manip
> 
> ### ** Examples
> 
>   set.seed(776655)
>   v1 <- rnorm(20)
>   v2 <- rnorm(20)
>   d1 <- sample(1:5,20,replace=TRUE)
>   d2 <- sample(1:4,20,replace=TRUE)
>   ldata <- cbind(v1,v2,d1,d2)
>   lc <- cat2bin(ldata,categorical=3:4)
> 
> 
> 
> cleanEx()
> nameEx("cdbw")
> ### * cdbw
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: cdbw
> ### Title: CDbw-index for cluster validation
> ### Aliases: cdbw
> ### Keywords: cluster
> 
> ### ** Examples
> 
>   options(digits=3)
>   iriss <- as.matrix(iris[c(1:5,51:55,101:105),-5])
>   irisc <- as.numeric(iris[c(1:5,51:55,101:105),5])
>   cdbw(iriss,irisc)
$cdbw
[1] 2.35

$cohesion
[1] 1.28

$compactness
[1] 1.33

$sep
[1] 1.38

> 
> 
> 
> cleanEx()
> nameEx("classifdist")
> ### * classifdist
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: classifdist
> ### Title: Classification of unclustered points
> ### Aliases: classifdist classifnp
> ### Keywords: cluster multivariate
> 
> ### ** Examples
>   
> set.seed(20000)
> x1 <- rnorm(50)
> y <- rnorm(100)
> x2 <- rnorm(40,mean=20)
> x3 <- rnorm(10,mean=25,sd=100)
> x <- cbind(c(x1,x2,x3),y)
> truec <- c(rep(1,50),rep(2,40),rep(3,10))
> topredict <- c(1,2,51,52,91)
> clumin <- truec
> clumin[topredict] <- -1
> 
> classifnp(x,clumin, method="averagedist")
  [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
 [38] 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
 [75] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 3 3 3 3 3 3 3 3 3
> classifnp(x,clumin, method="qda")
  [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
 [38] 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
 [75] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3
> classifdist(dist(x),clumin, centroids=c(3,53,93),method="centroid")
  [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
 [38] 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
 [75] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 3 3 3 3 3 3 3 3 3
> classifdist(dist(x),clumin,method="knn")
  [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
 [38] 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
 [75] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3
> 
> 
> 
> 
> cleanEx()
> nameEx("clucols")
> ### * clucols
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: clucols
> ### Title: Sets of colours and symbols for cluster plotting
> ### Aliases: clucols clugrey clusym
> ### Keywords: cluster
> 
> ### ** Examples
> 
>   set.seed(112233)
>   require(MASS)
Loading required package: MASS
>   require(flexmix)
Loading required package: flexmix
Loading required package: lattice
>   data(Cars93)
>   Cars934 <- Cars93[,c(3,5,8,10)]
>   cc <-
+     discrete.recode(Cars934,xvarsorted=FALSE,continuous=c(2,3),discrete=c(1,4))
>   fcc <- flexmix(cc$data~1,k=3,
+   model=lcmixed(continuous=2,discrete=2,ppdim=c(6,3),diagonal=TRUE))
>   plot(Cars934[,c(2,3)],col=clucols(3)[fcc@cluster],pch=clusym[fcc@cluster])
> 
> 
> 
> cleanEx()

detaching ‘package:flexmix’, ‘package:lattice’, ‘package:MASS’

> nameEx("clujaccard")
> ### * clujaccard
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: clujaccard
> ### Title: Jaccard similarity between logical vectors
> ### Aliases: clujaccard
> ### Keywords: cluster
> 
> ### ** Examples
> 
>   c1 <- rep(TRUE,10)
>   c2 <- c(FALSE,rep(TRUE,9))
>   clujaccard(c1,c2)
[1] 0.9
> 
> 
> 
> cleanEx()
> nameEx("clusexpect")
> ### * clusexpect
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: clusexpect
> ### Title: Expected value of the number of times a fixed point cluster is
> ###   found
> ### Aliases: clusexpect
> ### Keywords: univar cluster
> 
> ### ** Examples
> 
>   round(clusexpect(500,4,150,2000),digits=2)
[1] 1.36
> 
> 
> 
> cleanEx()
> nameEx("cluster.stats")
> ### * cluster.stats
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: cluster.stats
> ### Title: Cluster validation statistics
> ### Aliases: cluster.stats
> ### Keywords: cluster multivariate
> 
> ### ** Examples
>   
>   set.seed(20000)
>   options(digits=3)
>   face <- rFace(200,dMoNo=2,dNoEy=0,p=2)
>   dface <- dist(face)
>   complete3 <- cutree(hclust(dface),3)
>   cluster.stats(dface,complete3,
+                 alt.clustering=as.integer(attr(face,"grouping")))
$n
[1] 200

$cluster.number
[1] 3

$cluster.size
[1] 136  60   4

$min.cluster.size
[1] 4

$noisen
[1] 0

$diameter
[1] 10.80  5.76  9.00

$average.distance
[1] 3.03 2.21 7.05

$median.distance
[1] 2.84 1.48 8.32

$separation
[1] 5.87 5.87 7.22

$average.toother
[1] 13.8 13.0 20.8

$separation.matrix
      [,1] [,2]  [,3]
[1,]  0.00 5.87 14.98
[2,]  5.87 0.00  7.22
[3,] 14.98 7.22  0.00

$ave.between.matrix
     [,1] [,2] [,3]
[1,]  0.0 13.1 24.5
[2,] 13.1  0.0 12.2
[3,] 24.5 12.2  0.0

$average.between
[1] 13.7

$average.within
[1] 2.9

$n.between
[1] 8944

$n.within
[1] 10956

$max.diameter
[1] 10.8

$min.separation
[1] 5.87

$within.cluster.ss
[1] 1198

$clus.avg.silwidths
    1     2     3 
0.752 0.818 0.355 

$avg.silwidth
[1] 0.764

$g2
NULL

$g3
NULL

$pearsongamma
[1] 0.883

$dunn
[1] 0.544

$dunn2
[1] 1.73

$entropy
[1] 0.702

$wb.ratio
[1] 0.211

$ch
[1] 699

$cwidegap
[1] 1.81 1.21 8.32

$widestgap
[1] 8.32

$sindex
[1] 6.19

$corrected.rand
[1] 0.345

$vi
[1] 0.97

>   
> 
> 
> 
> cleanEx()
> nameEx("cluster.varstats")
> ### * cluster.varstats
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: cluster.varstats
> ### Title: Variablewise statistics for clusters
> ### Aliases: cluster.varstats print.varwisetables
> ### Keywords: cluster
> 
> ### ** Examples
> 
>   set.seed(112233)
>   options(digits=3)
>   require(MASS)
Loading required package: MASS
>   require(flexmix)
Loading required package: flexmix
Loading required package: lattice
>   data(Cars93)
>   Cars934 <- Cars93[,c(3,5,8,10)]
>   cc <-
+     discrete.recode(Cars934,xvarsorted=FALSE,continuous=c(2,3),discrete=c(1,4))
>   fcc <- flexmix(cc$data~1,k=2,
+   model=lcmixed(continuous=2,discrete=2,ppdim=c(6,3),diagonal=TRUE))
>   cv <-
+     cluster.varstats(fcc@cluster,Cars934, contdata=Cars934[,c(2,3)],
+     tablevar=c(1,4),catvar=c(2,3),quantvar=c(2,3),projmethod="awc",
+     ask=FALSE)

Cluster  1   29  out of  93  points.

Cluster  1   Type 
             Type
In cluster  1 Compact Large Midsize Small Sporty Van
        FALSE      10    11      22     0     12   9
        TRUE        6     0       0    21      2   0

Cluster  1   Price 
  Mean= 10.8  all obs.= 19.5 
  Standard deviation= 2.2  all obs.= 9.66 
  0%  25%  50%  75% 100% 
 7.4  9.1 10.9 11.8 16.5 
[1] "All obs.:"
  0%  25%  50%  75% 100% 
 7.4 12.2 17.7 23.3 61.9 
Cluster  1   MPG.highway 
  Mean= 34.8  all obs.= 29.1 
  Standard deviation= 5.12  all obs.= 5.33 
  0%  25%  50%  75% 100% 
  27   31   33   37   50 
[1] "All obs.:"
  0%  25%  50%  75% 100% 
  20   26   28   31   50 
Cluster  1   DriveTrain 
             DriveTrain
In cluster  1 4WD Front Rear
        FALSE   8    40   16
        TRUE    2    27    0


Cluster  2   64  out of  93  points.

Cluster  2   Type 
             Type
In cluster  2 Compact Large Midsize Small Sporty Van
        FALSE       6     0       0    21      2   0
        TRUE       10    11      22     0     12   9

Cluster  2   Price 
  Mean= 23.5  all obs.= 19.5 
  Standard deviation= 9.13  all obs.= 9.66 
  0%  25%  50%  75% 100% 
13.3 17.3 19.9 28.2 61.9 
[1] "All obs.:"
  0%  25%  50%  75% 100% 
 7.4 12.2 17.7 23.3 61.9 
Cluster  2   MPG.highway 
  Mean= 26.5  all obs.= 29.1 
  Standard deviation= 2.89  all obs.= 5.33 
  0%  25%  50%  75% 100% 
20.0 25.0 26.0 28.2 32.0 
[1] "All obs.:"
  0%  25%  50%  75% 100% 
  20   26   28   31   50 
Cluster  2   DriveTrain 
             DriveTrain
In cluster  2 4WD Front Rear
        FALSE   2    27    0
        TRUE    8    40   16

>   print(cv)
       Type
Cluster Compact Large Midsize Small Sporty Van
    1         6     0       0    21      2   0
    2        10    11      22     0     12   9
    Sum      16    11      22    21     14   9
 
       Categorised  Price
Cluster  1  2  3  4  5  6  7  8  9 10
    1   10  9  8  0  2  0  0  0  0  0
    2    0  0  1  9  8  9  9  9  9 10
    Sum 10  9  9  9 10  9  9  9  9 10
 
       Categorised  MPG.highway
Cluster  1  2  3  4  5  6  7  8  9 10
    1    0  0  0  1  0  4  3  7  6  8
    2   10 12 11  5 10 11  4  1  0  0
    Sum 10 12 11  6 10 15  7  8  6  8
 
       DriveTrain
Cluster 4WD Front Rear
    1     2    27    0
    2     8    40   16
    Sum  10    67   16
 
> 
> 
> 
> cleanEx()

detaching ‘package:flexmix’, ‘package:lattice’, ‘package:MASS’

> nameEx("clusterboot")
> ### * clusterboot
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: clusterboot
> ### Title: Clusterwise cluster stability assessment by resampling
> ### Aliases: clusterboot print.clboot plot.clboot
> ### Keywords: cluster multivariate
> 
> ### ** Examples
> 
>   options(digits=3)
>   set.seed(20000)
>   face <- rFace(50,dMoNo=2,dNoEy=0,p=2)
>   cf1 <- clusterboot(face,B=3,bootmethod=
+           c("boot","noise","jitter"),clustermethod=kmeansCBI,
+           krange=5,seed=15555)
boot 1 
boot 2 
boot 3 
noise 1 
noise 2 
noise 3 
jitter 1 
jitter 2 
jitter 3 
>   print(cf1)
* Cluster stability assessment *
Cluster method:  kmeans 
Full clustering results are given as parameter result
of the clusterboot object, which also provides further statistics
of the resampling results.
Number of resampling runs:  3 

Number of clusters found in data:  5 

 Clusterwise Jaccard bootstrap (omitting multiple points) mean:
[1] 1.000 0.952 0.714 1.000 0.929
dissolved:
[1] 0 0 1 0 0
recovered:
[1] 3 3 2 3 3
 Clusterwise Jaccard replacement by noise mean:
[1] 0.197 0.796 0.066 0.570 0.671
dissolved:
[1] 3 0 3 1 1
recovered:
[1] 0 2 0 1 2
 Clusterwise Jaccard jittering mean:
[1] 0.583 0.889 0.412 0.530 0.926
dissolved:
[1] 2 0 2 1 0
recovered:
[1] 1 3 1 0 3
>   plot(cf1)
>   cf2 <- clusterboot(dist(face),B=3,bootmethod=
+           "subset",clustermethod=disthclustCBI,
+           k=5, cut="number", method="average", showplots=TRUE, seed=15555)
subset 1 
subset 2 
subset 3 
>   print(cf2)
* Cluster stability assessment *
Cluster method:  hclust 
Full clustering results are given as parameter result
of the clusterboot object, which also provides further statistics
of the resampling results.
Number of resampling runs:  3 

Number of clusters found in data:  5 

 Clusterwise Jaccard subsetting mean:
[1] 0.716 0.857 0.333 0.333 1.000
dissolved:
[1] 1 0 2 2 0
recovered:
[1] 1 2 1 1 3
>   d1 <- c("a","b","a","c")
>   d2 <- c("a","a","a","b")
>   dx <- as.data.frame(cbind(d1,d2))
>   cpx <- clusterboot(dx,k=2,B=10,clustermethod=claraCBI,
+           multipleboot=TRUE,usepam=TRUE,datatomatrix=FALSE)
boot 1 
boot 2 
boot 3 
boot 4 
boot 5 
boot 6 
boot 7 
boot 8 
boot 9 
boot 10 
>   print(cpx)
* Cluster stability assessment *
Cluster method:  clara/pam 
Full clustering results are given as parameter result
of the clusterboot object, which also provides further statistics
of the resampling results.
Number of resampling runs:  10 

Number of clusters found in data:  2 

 Clusterwise Jaccard bootstrap mean:
[1] 0.95 0.80
dissolved:
[1] 0 2
recovered:
[1] 8 8
> 
> 
> 
> cleanEx()
> nameEx("cmahal")
> ### * cmahal
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: cmahal
> ### Title: Generation of tuning constant for Mahalanobis fixed point
> ###   clusters.
> ### Aliases: cmahal
> ### Keywords: cluster
> 
> ### ** Examples
> 
>   plot(1:100,cmahal(100,3,nmin=5,cmin=qchisq(0.99,3),nc1=90),
+        xlab="FPC size", ylab="cmahal")
> 
> 
> 
> cleanEx()
> nameEx("concomp")
> ### * concomp
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: con.comp
> ### Title: Connectivity components of an undirected graph
> ### Aliases: con.comp
> ### Keywords: array cluster
> 
> ### ** Examples
> 
>   set.seed(1000)
>   x <- rnorm(20)
>   m <- matrix(0,nrow=20,ncol=20)
>   for(i in 1:20)
+     for(j in 1:20)
+       m[i,j] <- abs(x[i]-x[j])
>   d <- m<0.2
>   cc <- con.comp(d)
>   max(cc) # number of connectivity components
[1] 6
>   plot(x,cc)
>   # The same should be produced by
>   # cutree(hclust(as.dist(m),method="single"),h=0.2).
> 
> 
> 
> cleanEx()
> nameEx("confusion")
> ### * confusion
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: confusion
> ### Title: Misclassification probabilities in mixtures
> ### Aliases: confusion
> ### Keywords: cluster multivariate
> 
> ### ** Examples
> 
>   set.seed(12345)
>   m <- rpois(20,lambda=5)
>   dim(m) <- c(5,4)
>   pro <- apply(m,2,sum)
>   pro <- pro/sum(pro)
>   m <- m/apply(m,1,sum)
>   round(confusion(m,pro,1,2),digits=2)
[1] 0.7
> 
> 
> 
> cleanEx()
> nameEx("cov.wml")
> ### * cov.wml
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: cov.wml
> ### Title: Weighted Covariance Matrices (Maximum Likelihood)
> ### Aliases: cov.wml
> ### Keywords: multivariate
> 
> ### ** Examples
> 
>   x <- c(1,2,3,4,5,6,7,8,9,10)
>   y <- c(1,2,3,8,7,6,5,8,9,10)
>   cov.wml(cbind(x,y),wt=c(0,0,0,1,1,1,1,1,0,0))
$cov
     x     y
x  2.0 -0.40
y -0.4  1.36

$center
  x   y 
6.0 6.8 

$n.obs
[1] 10

$wt
 [1] 0.0 0.0 0.0 0.2 0.2 0.2 0.2 0.2 0.0 0.0

>   cov.wt(cbind(x,y),wt=c(0,0,0,1,1,1,1,1,0,0))
$cov
     x    y
x  2.5 -0.5
y -0.5  1.7

$center
  x   y 
6.0 6.8 

$n.obs
[1] 10

$wt
 [1] 0.0 0.0 0.0 0.2 0.2 0.2 0.2 0.2 0.0 0.0

> 
> 
> 
> cleanEx()
> nameEx("cvnn")
> ### * cvnn
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: cvnn
> ### Title: Cluster validation based on nearest neighbours
> ### Aliases: cvnn
> ### Keywords: cluster
> 
> ### ** Examples
> 
>   options(digits=3)
>   iriss <- as.matrix(iris[c(1:10,51:55,101:105),-5])
>   irisc <- as.numeric(iris[c(1:10,51:55,101:105),5])
>   print(cvnn(dist(iriss),list(irisc,rep(1:4,5))))
$cvnnindex
[1] 0.616 2.000

$sep
[1] 0.36 0.92

$comp
[1] 0.674 2.998

> 
> 
> 
> cleanEx()
> nameEx("cweight")
> ### * cweight
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: cweight
> ### Title: Weight function for AWC
> ### Aliases: cweight
> ### Keywords: arith
> 
> ### ** Examples
> 
>   cweight(4,1)
[1] 0.25
> 
> 
> 
> cleanEx()
> nameEx("dbscan")
> ### * dbscan
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: dbscan
> ### Title: DBSCAN density reachability and connectivity clustering
> ### Aliases: dbscan print.dbscan plot.dbscan predict.dbscan
> ### Keywords: multivariate cluster
> 
> ### ** Examples
> 
>   set.seed(665544)
>   n <- 600
>   x <- cbind(runif(10, 0, 10)+rnorm(n, sd=0.2), runif(10, 0, 10)+rnorm(n,
+     sd=0.2))
>   par(bg="grey40")
>   ds <- dbscan(x, 0.2)
> # run with showplot=1 to see how dbscan works.
>   ds
dbscan Pts=600 MinPts=5 eps=0.2
        0  1  2  3  4  5  6  7  8  9 10 11
border 28  4  4  8  5  3  3  4  3  4  6  4
seed    0 50 53 51 52 51 54 54 54 53 51  1
total  28 54 57 59 57 54 57 58 57 57 57  5
>   plot(ds, x)
> 
>   x2 <- matrix(0,nrow=4,ncol=2)
>   x2[1,] <- c(5,2)
>   x2[2,] <- c(8,3)
>   x2[3,] <- c(4,4)
>   x2[4,] <- c(9,9)
>   predict(ds, x, x2)
[1] 4 9 0 0
> 
>   n <- 600
>   x <- cbind((1:3)+rnorm(n, sd=0.2), (1:3)+rnorm(n, sd=0.2))
> 
> # Not run, but results from my machine are 0.105 - 0.068 - 0.255:
> #  system.time(ds <- dbscan(x, 0.3, countmode=NULL, method="raw"))[3] 
> #  system.time(dsb <- dbscan(x, 0.3, countmode=NULL, method="hybrid"))[3]
> #  system.time(dsc <- dbscan(dist(x), 0.3, countmode=NULL,
> #    method="dist"))[3]
> 
> 
> 
> graphics::par(get("par.postscript", pos = 'CheckExEnv'))
> cleanEx()
> nameEx("dipp.tantrum")
> ### * dipp.tantrum
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: dipp.tantrum
> ### Title: Simulates p-value for dip test
> ### Aliases: dipp.tantrum
> ### Keywords: cluster
> 
> ### ** Examples
> 
> # not run, requires package diptest
> #  x <- runif(100)
> #  d <- dip(x)
> #  dt <- dipp.tantrum(x,d,M=10)
> 
> 
> 
> cleanEx()
> nameEx("diptest.multi")
> ### * diptest.multi
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: diptest.multi
> ### Title: Diptest for discriminant coordinate projection
> ### Aliases: diptest.multi
> ### Keywords: cluster multivariate
> 
> ### ** Examples
> 
>   require(diptest)
Loading required package: diptest
>   x <- cbind(runif(100),runif(100))
>   partition <- 1+(x[,1]<0.5)
>   d1 <- diptest.multi(x,partition)
>   d2 <- diptest.multi(x,partition,pvalue="tantrum",M=10)
> 
> 
> 
> cleanEx()

detaching ‘package:diptest’

> nameEx("discrcoord")
> ### * discrcoord
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: discrcoord
> ### Title: Discriminant coordinates/canonical variates
> ### Aliases: discrcoord
> ### Keywords: multivariate classif
> 
> ### ** Examples
> 
>   set.seed(4634)
>   face <- rFace(600,dMoNo=2,dNoEy=0)
>   grface <- as.integer(attr(face,"grouping"))
>   dcf <- discrcoord(face,grface)
>   plot(dcf$proj,col=grface)
>   # ...done in one step by function plotcluster.
> 
> 
> 
> cleanEx()
> nameEx("discrete.recode")
> ### * discrete.recode
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: discrete.recode
> ### Title: Recodes mixed variables dataset
> ### Aliases: discrete.recode
> ### Keywords: manip
> 
> ### ** Examples
> 
>   set.seed(776655)
>   v1 <- rnorm(20)
>   v2 <- rnorm(20)
>   d1 <- sample(c(2,4,6,8),20,replace=TRUE)
>   d2 <- sample(1:4,20,replace=TRUE)
>   ldata <- cbind(v1,d1,v2,d2)
>   lc <-
+   discrete.recode(ldata,xvarsorted=FALSE,continuous=c(1,3),discrete=c(2,4))
>   require(MASS)
Loading required package: MASS
>   data(Cars93)
>   Cars934 <- Cars93[,c(3,5,8,10)]
>   cc <- discrete.recode(Cars934,xvarsorted=FALSE,continuous=c(2,3),discrete=c(1,4))
> 
> 
> 
> cleanEx()

detaching ‘package:MASS’

> nameEx("discrproj")
> ### * discrproj
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: discrproj
> ### Title: Linear dimension reduction for classification
> ### Aliases: discrproj
> ### Keywords: multivariate classif
> 
> ### ** Examples
> 
> set.seed(4634)
> face <- rFace(300,dMoNo=2,dNoEy=0,p=3)
> grface <- as.integer(attr(face,"grouping"))
> 
> # The abs in the following is there to unify the output,
> # because eigenvectors are defined only up to their sign.
> # Statistically it doesn't make sense to compute absolute values. 
> round(abs(discrproj(face,grface, method="nc")$units),digits=2)
     [,1] [,2] [,3]
[1,] 0.84 1.15 0.02
[2,] 0.34 0.28 0.01
[3,] 0.07 0.04 1.00
> round(abs(discrproj(face,grface, method="wnc")$units),digits=2)
     [,1] [,2] [,3]
[1,] 0.07 1.42 0.04
[2,] 0.44 0.00 0.01
[3,] 0.04 0.00 1.01
> round(abs(discrproj(face,grface, clnum=1, method="arc")$units),digits=2)
     [,1] [,2] [,3]
[1,] 1.30 0.50 0.34
[2,] 0.65 0.05 0.08
[3,] 0.08 0.66 0.47
> 
> 
> 
> cleanEx()
> nameEx("distancefactor")
> ### * distancefactor
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: distancefactor
> ### Title: Factor for dissimilarity of mixed type data
> ### Aliases: distancefactor
> ### Keywords: cluster
> 
> ### ** Examples
> 
>   set.seed(776655)
>   d1 <- sample(1:5,20,replace=TRUE)
>   d2 <- sample(1:4,20,replace=TRUE)
>   ldata <- cbind(d1,d2)
>   lc <- cat2bin(ldata,categorical=1)$data
>   lc[,1:5] <- lc[,1:5]*distancefactor(5,20,type="categorical")
>   lc[,6] <- lc[,6]*distancefactor(4,20,type="ordinal")
> 
> 
> 
> cleanEx()
> nameEx("distcritmulti")
> ### * distcritmulti
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: distcritmulti
> ### Title: Distance based validity criteria for large data sets
> ### Aliases: distcritmulti
> ### Keywords: cluster
> 
> ### ** Examples
> 
>     set.seed(20000)
>     options(digits=3)
>     face <- rFace(50,dMoNo=2,dNoEy=0,p=2)
>     clustering <- as.integer(attr(face,"grouping"))
>     distcritmulti(face,clustering,ns=3,seed=100000,criterion="pearsongamma")
$crit.overall
[1] 0.464

$crit.sub
[1] 0.452 0.488 0.452

$crit.sd
[1] 0.0209

$subsets
$subsets[[1]]
 [1] 34 24 32 45  5 22  8 18 41  4 30 11 13  1 27 40

$subsets[[2]]
 [1] 46 39 50 42 23 19 29 20 43 47 16  6 28  9 38 12

$subsets[[3]]
 [1] 36 10  2 44  3 35 17 33 21 31  7 25 37 14 26 15 48 49


> 
> 
> 
> cleanEx()
> nameEx("dridgeline")
> ### * dridgeline
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: dridgeline
> ### Title: Density along the ridgeline
> ### Aliases: dridgeline
> ### Keywords: cluster multivariate
> 
> ### ** Examples
> 
>   q <- dridgeline(seq(0,1,0.1),0.5,c(1,1),c(2,5),diag(2),diag(2))
> 
> 
> 
> cleanEx()
> nameEx("dudahart2")
> ### * dudahart2
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: dudahart2
> ### Title: Duda-Hart test for splitting
> ### Aliases: dudahart2
> ### Keywords: cluster
> 
> ### ** Examples
> 
>   options(digits=2)
>   set.seed(98765)
>   iriss <- iris[sample(150,20),-5]
>   km <- kmeans(iriss,2)
>   dudahart2(iriss,km$cluster)
$p.value
[1] 2.4e-06

$dh
[1] 0.19

$compare
[1] 0.4

$cluster1
[1] FALSE

$alpha
[1] 0.001

$z
[1] 3.1

> 
> 
> 
> cleanEx()
> nameEx("extract.mixturepars")
> ### * extract.mixturepars
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: extract.mixturepars
> ### Title: Extract parameters for certain components from mclust
> ### Aliases: extract.mixturepars
> ### Keywords: cluster multivariate
> 
> ### ** Examples
> 
>   set.seed(98765)
>   options(digits=2)
>   require(mclust)
Loading required package: mclust
Package 'mclust' version 5.4
Type 'citation("mclust")' for citing this R package in publications.
>   iriss <- iris[sample(150,20),-5]
>   irisBIC <- mclustBIC(iriss)
>   siris <- summary(irisBIC,iriss)
>   extract.mixturepars(siris,2)
$pro
[1] 1

$mean
Sepal.Length  Sepal.Width Petal.Length  Petal.Width 
        4.96         3.25         1.45         0.26 

$variance
$variance$modelName
[1] "VEV"

$variance$d
[1] 4

$variance$G
[1] 1

$variance$sigma
             Sepal.Length Sepal.Width Petal.Length Petal.Width
Sepal.Length      0.08907     0.08652       0.0096     0.00016
Sepal.Width       0.08652     0.14808       0.0302    -0.00072
Petal.Length      0.00955     0.03022       0.0285     0.00874
Petal.Width       0.00016    -0.00072       0.0087     0.00653

$variance$scale
[1] 0.134 0.024

$variance$shape
[1] 9.138 1.541 0.810 0.088

$variance$orientation
             Sepal.Length Sepal.Width Petal.Length Petal.Width
Sepal.Length       0.5676        0.65         0.50       -0.12
Sepal.Width        0.8076       -0.32        -0.47        0.16
Petal.Length       0.1603       -0.67         0.58       -0.43
Petal.Width        0.0044       -0.19         0.44        0.88


> 
> 
> 
> cleanEx()

detaching ‘package:mclust’

> nameEx("findrep")
> ### * findrep
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: findrep
> ### Title: Finding representatives for cluster border
> ### Aliases: findrep
> ### Keywords: cluster
> 
> ### ** Examples
> 
>   options(digits=3)
>   iriss <- as.matrix(iris[c(1:5,51:55,101:105),-5])
>   irisc <- as.numeric(iris[c(1:5,51:55,101:105),5])
>   findrep(iriss,colMeans(iriss),irisc,cluster=1,r=2)
$repc
[1] 3 5

$repx
[1] 3 5

$maxr
[1] 2

$wvar
[1] 10.6

> 
> 
> 
> cleanEx()
> nameEx("fixmahal")
> ### * fixmahal
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: fixmahal
> ### Title: Mahalanobis Fixed Point Clusters
> ### Aliases: fixmahal summary.mfpc plot.mfpc fpclusters.mfpc
> ###   print.summary.mfpc print.mfpc fpmi
> ### Keywords: cluster multivariate robust
> 
> ### ** Examples
> 
>   options(digits=2)
>   set.seed(20000)
>   face <- rFace(400,dMoNo=2,dNoEy=0, p=3)
>   # The first example uses grouping information via init.group.
>   initg <- list()
>   grface <- as.integer(attr(face,"grouping"))
>   for (i in 1:5) initg[[i]] <- (grface==i)
>   ff0 <- fixmahal(face, pointit=FALSE, init.group=initg)
>   summary(ff0)
  *  Mahalanobis Fixed Point Clusters  *

Often a clear cluster in the data leads to several similar FPCs.
The summary shows the representative FPCs of groups of similar FPCs.

Method  fuzzy  was used.
Number of representative FPCs:  5 

FPCs with less than  10  points were skipped.
0  iteration runs led to  0  skipped clusters.
  Weight 1 for r^2<=  7.8  weight 0 for r^2>  13  
  Constant ca=  7.8  corresponding to alpha=  0.95 

 FPC  1 
  Times found (group members):  1 
  Mean:
[1] -2.1 17.1  1.2
  Covariance matrix:
        [,1]    [,2]   [,3]
[1,]  0.1420 -0.0053 -0.041
[2,] -0.0053  0.1518  0.063
[3,] -0.0413  0.0632  1.057
  Number of points (sum of weights):  39 

 FPC  2 
  Times found (group members):  1 
  Mean:
[1]  2.0 17.0  1.2
  Covariance matrix:
       [,1]   [,2]  [,3]
[1,] 0.1544 0.0038 0.043
[2,] 0.0038 0.1159 0.057
[3,] 0.0427 0.0567 1.296
  Number of points (sum of weights):  76 

 FPC  3 
  Times found (group members):  1 
  Mean:
[1] -0.0043  3.0912  0.5582
  Covariance matrix:
       [,1]   [,2]   [,3]
[1,] 0.1866 0.0017 0.0175
[2,] 0.0017 0.0438 0.0047
[3,] 0.0175 0.0047 0.2097
  Number of points (sum of weights):  96 

 FPC  4 
  Times found (group members):  2 
  Mean:
[1] 0.013 3.880 0.615
  Covariance matrix:
       [,1]   [,2]  [,3]
[1,] 0.2064 0.0067 0.017
[2,] 0.0067 4.8776 0.193
[3,] 0.0173 0.1934 0.262
  Number of points (sum of weights):  197 

 FPC  5 
  Times found (group members):  1 
  Mean:
[1] 0.11 7.58 0.63
  Covariance matrix:
      [,1]   [,2]  [,3]
[1,] 1.830  1.769 0.037
[2,] 1.769 36.251 0.074
[3,] 0.037  0.074 0.270
  Number of points (sum of weights):  328 

Number of points (rounded weights) in intersection of representative FPCs
     [,1] [,2] [,3] [,4] [,5]
[1,]   39    0    0    0   27
[2,]    0   76    0    0   58
[3,]    0    0   96   96   96
[4,]    0    0   96  197  197
[5,]   27   58   96  197  328
>   cff0 <- fpclusters(ff0)
>   plot(face, col=1+cff0[[1]])
>   plot(face, col=1+cff0[[4]]) # Why does this come out as a cluster? 
>   plot(ff0, face, 4) # A bit clearer...
>   # Without grouping information, examples need more time:
>   # ff1 <- fixmahal(face)
>   # summary(ff1)
>   # cff1 <- fpclusters(ff1)
>   # plot(face, col=1+cff1[[1]])
>   # plot(face, col=1+cff1[[6]]) # Why does this come out as a cluster? 
>   # plot(ff1, face, 6) # A bit clearer...
>   # ff2 <- fixmahal(face,method="ml")
>   # summary(ff2)
>   # ff3 <- fixmahal(face,method="ml",calpha=0.95,subset=50)
>   # summary(ff3)
>   ## ...fast, but lots of clusters. mer=0.3 may be useful here.
>   # set.seed(3000)
>   # face2 <- rFace(400,dMoNo=2,dNoEy=0)
>   # ff5 <- fixmahal(face2)
>   # summary(ff5)
>   ## misses right eye of face data; with p=6,
>   ## initial configurations are too large for 40 point clusters 
>   # ff6 <- fixmahal(face2, startn=30)
>   # summary(ff6)
>   # cff6 <- fpclusters(ff6)
>   # plot(face2, col=1+cff6[[3]])
>   # plot(ff6, face2, 3)
>   # x <- c(1,2,3,6,6,7,8,120)
>   # ff8 <- fixmahal(x)
>   # summary(ff8)
>   # ...dataset a bit too small for the defaults...
>   # ff9 <- fixmahal(x, mnc=3, startn=3)
>   # summary(ff9)
> 
> 
> 
> cleanEx()
> nameEx("fixreg")
> ### * fixreg
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: fixreg
> ### Title: Linear Regression Fixed Point Clusters
> ### Aliases: fixreg summary.rfpc plot.rfpc fpclusters.rfpc
> ###   print.summary.rfpc print.rfpc rfpi
> ### Keywords: cluster robust regression
> 
> ### ** Examples
> 
> set.seed(190000)
> options(digits=3)
> data(tonedata)
> attach(tonedata)
> tonefix <- fixreg(stretchratio,tuned,mtf=1,ir=20)
> summary(tonefix)
  *  Fixed Point Clusters  *

Often a clear cluster in the data leads to several similar FPCs.
The summary shows the representative FPCs of groups of similar FPCs,
which were found at least  1  times.

Constant ca=  10.1 
Number of representative FPCs:  3 

FPCs with less than  50  points were skipped.
0  iterations led to skipped FPCs.

 FPC  1 
  Times found (group members):  17 
  Ratio to estimated expectation:  1.59 
  Regression parameters:
Intercept         X 
   1.9051    0.0477 
  Error variance:  0.00282 
  Number of points:  122 

 FPC  2 
  Times found (group members):  3 
  Ratio to estimated expectation:  2.08 
  Regression parameters:
Intercept         X 
  0.00351   0.99865 
  Error variance:  3.69e-05 
  Number of points:  63 

 FPC  3 
  Times found (group members):  1 
  Ratio to estimated expectation:  0.602 
  Regression parameters:
Intercept         X 
    0.794     0.602 
  Error variance:  0.00112 
  Number of points:  66 

Number of points in intersection of  representative FPCs
     [,1] [,2] [,3]
[1,]  122   46   61
[2,]   46   63   47
[3,]   61   47   66
> # This is designed to have a fast example; default setting would be better.
> # If you want to see more (and you have a bit more time),
> # try out the following:
> ## Not run: 
> ##D  set.seed(1000)
> ##D  tonefix <- fixreg(stretchratio,tuned)
> ##D # Default - good for these data
> ##D  summary(tonefix)
> ##D  plot(tonefix,stretchratio,tuned,1)
> ##D  plot(tonefix,stretchratio,tuned,2)
> ##D  plot(tonefix,stretchratio,tuned,3,bw=FALSE,pch=5) 
> ##D  toneclus <- fpclusters(tonefix,stretchratio,tuned)
> ##D  plot(stretchratio,tuned,col=1+toneclus[[2]])
> ##D  tonefix2 <- fixreg(stretchratio,tuned,distcut=1,mtf=1,countmode=50)
> ##D # Every found fixed point cluster is reported,
> ##D # no matter how instable it may be.
> ##D  summary(tonefix2)
> ##D  tonefix3 <- fixreg(stretchratio,tuned,ca=7)
> ##D # ca defaults to 10.07 for these data.
> ##D  summary(tonefix3)
> ##D  subset <- c(rep(FALSE,5),rep(TRUE,24),rep(FALSE,121))
> ##D  tonefix4 <- fixreg(stretchratio,tuned,
> ##D                     mtf=1,ir=0,init.group=list(subset))
> ##D  summary(tonefix4)
> ## End(Not run)
> 
> 
> 
> cleanEx()

detaching ‘tonedata’

> nameEx("flexmixedruns")
> ### * flexmixedruns
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: flexmixedruns
> ### Title: Fitting mixed Gaussian/multinomial mixtures with flexmix
> ### Aliases: flexmixedruns
> ### Keywords: cluster
> 
> ### ** Examples
> 
>   options(digits=3)
>   set.seed(776655)
>   v1 <- rnorm(100)
>   v2 <- rnorm(100)
>   d1 <- sample(1:5,100,replace=TRUE)
>   d2 <- sample(1:4,100,replace=TRUE)
>   ldata <- cbind(v1,v2,d1,d2)
>   fr <- flexmixedruns(ldata,
+     continuous=2,discrete=2,simruns=2,n.cluster=2:3,allout=FALSE)
k=  2  new best fit found in run  1 
k=  2  new best fit found in run  2 
k=  2  BIC=  1258 
k=  3  new best fit found in run  1 
k=  3  new best fit found in run  2 
k=  3  BIC=  1300 
>   print(fr$optimalk)
[1] 2
>   print(fr$optsummary)

Call:
flexmix(formula = x ~ 1, k = k, cluster = initial.cluster, model = lcmixed(continuous = continuous, 
    discrete = discrete, ppdim = ppdim, diagonal = diagonal), 
    control = control)

       prior size post>0 ratio
Comp.1 0.527   51    100  0.51
Comp.2 0.473   49    100  0.49

'log Lik.' -576 (df=23)
AIC: 1198   BIC: 1258 

>   print(fr$flexout@cluster)
  [1] 2 1 1 1 2 2 1 2 2 1 1 2 2 1 2 2 1 2 1 1 1 1 2 1 2 1 1 2 2 1 1 2 2 1 2 1 1
 [38] 2 2 1 2 1 1 2 2 1 1 2 1 2 1 2 1 1 2 2 1 2 2 2 1 2 1 1 2 1 1 2 2 1 1 2 2 1
 [75] 1 1 1 2 2 1 2 1 1 1 1 2 1 1 2 1 1 2 1 2 2 2 2 2 2 2
>   print(fr$flexout@components)
$Comp.1
$Comp.1[[1]]
$center
[1] -0.0636 -0.5109

$cov
     [,1] [,2]
[1,] 1.12  0.0
[2,] 0.00  1.1

$pp
$pp[[1]]
[1] 0.273 0.226 0.157 0.298 0.046

$pp[[2]]
[1] 0.149 0.172 0.506 0.173




$Comp.2
$Comp.2[[1]]
$center
[1] 0.246 0.423

$cov
      [,1]  [,2]
[1,] 0.674 0.000
[2,] 0.000 0.855

$pp
$pp[[1]]
[1] 0.0974 0.1707 0.2691 0.0482 0.4145

$pp[[2]]
[1] 0.2782 0.4220 0.0484 0.2514




> 
> 
> 
> cleanEx()
> nameEx("itnumber")
> ### * itnumber
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: itnumber
> ### Title: Number of regression fixed point cluster iterations
> ### Aliases: itnumber
> ### Keywords: univar cluster
> 
> ### ** Examples
> 
>   itnumber(500,4,150,2)
[1] 6985
> 
> 
> 
> cleanEx()
> nameEx("jittervar")
> ### * jittervar
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: jittervar
> ### Title: Jitter variables in a data matrix
> ### Aliases: jittervar
> ### Keywords: manip
> 
> ### ** Examples
> 
>   set.seed(776655)
>   v1 <- rnorm(20)
>   v2 <- rnorm(20)
>   d1 <- sample(1:5,20,replace=TRUE)
>   d2 <- sample(1:4,20,replace=TRUE)
>   ldata <- cbind(v1,v2,d1,d2)
>   jv <- jittervar(ldata,jitterv=3:4)
> 
> 
> 
> cleanEx()
> nameEx("kmeansCBI")
> ### * kmeansCBI
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: kmeansCBI
> ### Title: Interface functions for clustering methods
> ### Aliases: kmeansCBI hclustCBI hclusttreeCBI disthclustCBI
> ###   disthclusttreeCBI noisemclustCBI distnoisemclustCBI claraCBI pamkCBI
> ###   trimkmeansCBI disttrimkmeansCBI dbscanCBI mahalCBI mergenormCBI
> ###   speccCBI tclustCBI pdfclustCBI
> ### Keywords: cluster multivariate
> 
> ### ** Examples
> 
>   options(digits=3)
>   set.seed(20000)
>   face <- rFace(50,dMoNo=2,dNoEy=0,p=2)
>   dbs <- dbscanCBI(face,eps=1.5,MinPts=4)
>   dhc <- disthclustCBI(dist(face),method="average",k=1.5,noisecut=2)
>   table(dbs$partition,dhc$partition)
   
     1
  1 16
  2  9
  3  9
  4  5
  5 11
>   dm <- mergenormCBI(face,G=10,modelNames="EEE",nnk=2)
>   dtc <- tclustCBI(face,6,trim=0.1,restr.fact=500)
>   table(dm$partition,dtc$partition)
   
     1  2  3  4  5  6  7
  1  4  0  0  1  0  0  0
  2 10  8  7  2  5  0  0
  3  0  1  0  0  0  0  0
  4  0  0  2  4  0  1  5
> 
> 
> 
> 
> cleanEx()
> nameEx("kmeansruns")
> ### * kmeansruns
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: kmeansruns
> ### Title: k-means with estimating k and initialisations
> ### Aliases: kmeansruns
> ### Keywords: cluster multivariate
> 
> ### ** Examples
> 
>   options(digits=3)
>   set.seed(20000)
>   face <- rFace(50,dMoNo=2,dNoEy=0,p=2)
>   pka <- kmeansruns(face,krange=1:5,critout=TRUE,runs=2,criterion="asw")
2  clusters  0.742 
3  clusters  0.746 
4  clusters  0.59 
5  clusters  0.598 
>   pkc <- kmeansruns(face,krange=1:5,critout=TRUE,runs=2,criterion="ch")
2  clusters  181 
3  clusters  108 
4  clusters  231 
5  clusters  187 
> 
> 
> 
> cleanEx()
> nameEx("lcmixed")
> ### * lcmixed
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: lcmixed
> ### Title: flexmix method for mixed Gaussian/multinomial mixtures
> ### Aliases: lcmixed
> ### Keywords: cluster
> 
> ### ** Examples
> 
>   set.seed(112233)
>   options(digits=3)
>   require(MASS)
Loading required package: MASS
>   require(flexmix)
Loading required package: flexmix
Loading required package: lattice
>   data(Cars93)
>   Cars934 <- Cars93[,c(3,5,8,10)]
>   cc <-
+   discrete.recode(Cars934,xvarsorted=FALSE,continuous=c(2,3),discrete=c(1,4))
>   fcc <- flexmix(cc$data~1,k=2,
+   model=lcmixed(continuous=2,discrete=2,ppdim=c(6,3),diagonal=TRUE))
>   summary(fcc)

Call:
flexmix(formula = cc$data ~ 1, k = 2, model = lcmixed(continuous = 2, 
    discrete = 2, ppdim = c(6, 3), diagonal = TRUE))

       prior size post>0 ratio
Comp.1 0.327   29     40 0.725
Comp.2 0.673   64     72 0.889

'log Lik.' -782 (df=23)
AIC: 1610   BIC: 1669 

> 
> 
> 
> cleanEx()

detaching ‘package:flexmix’, ‘package:lattice’, ‘package:MASS’

> nameEx("localshape")
> ### * localshape
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: localshape
> ### Title: Local shape matrix
> ### Aliases: localshape
> ### Keywords: multivariate
> 
> ### ** Examples
> 
>   options(digits=3)
>   data(iris)
>   localshape(iris[,-5],mscatter="cov")
             Sepal.Length Sepal.Width Petal.Length Petal.Width
Sepal.Length       631889      309217       265161       63257
Sepal.Width        309217      456220        34488       52444
Petal.Length       265161       34488       386520      116040
Petal.Width         63257       52444       116040      104828
> 
> 
> 
> cleanEx()
> nameEx("mahalanodisc")
> ### * mahalanodisc
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: mahalanodisc
> ### Title: Mahalanobis for AWC
> ### Aliases: mahalanodisc
> ### Keywords: multivariate
> 
> ### ** Examples
> 
>   options(digits=3)
>   x <- cbind(rnorm(50),rnorm(50))
>   mahalanodisc(x,c(0,0),cov(x))
 [1] 0.7135 0.4376 1.1082 4.8732 2.3944 5.0023 0.4707 1.8782 0.8586 0.1586
[11] 9.8187 0.2204 1.0252 7.1017 2.3418 0.0401 3.4794 3.7170 1.0145 5.6731
[21] 1.5074 1.3702 0.4105 6.8458 2.1580 0.0936 0.2516 3.1341 0.3337 0.6000
[31] 2.9447 0.0362 1.7431 2.4890 3.0445 0.3544 1.3904 0.1056 1.9390 0.9399
[41] 0.3619 1.6200 2.2192 1.0100 3.2679 1.0197 1.8863 1.1636 1.6318 1.3235
>   mahalanodisc(x,c(0,0),matrix(0,ncol=2,nrow=2))
 [1] 5.51e+09 4.08e+09 8.15e+09 3.82e+10 2.16e+10 4.60e+10 3.72e+09 1.64e+10
 [9] 6.56e+09 1.12e+09 8.05e+10 1.54e+09 8.62e+09 4.91e+10 1.82e+10 3.77e+08
[17] 3.26e+10 3.04e+10 6.98e+09 5.07e+10 1.07e+10 1.12e+10 3.79e+09 4.83e+10
[25] 1.96e+10 8.81e+08 2.21e+09 2.16e+10 2.34e+09 5.22e+09 2.17e+10 2.88e+08
[33] 1.54e+10 2.32e+10 2.25e+10 2.83e+09 1.29e+10 9.60e+08 1.35e+10 6.54e+09
[41] 3.21e+09 1.52e+10 1.83e+10 8.00e+09 2.99e+10 8.12e+09 1.76e+10 9.19e+09
[49] 1.51e+10 1.00e+10
> 
> 
> 
> cleanEx()
> nameEx("mahalanofix")
> ### * mahalanofix
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: mahalanofix
> ### Title: Mahalanobis distances from center of indexed points
> ### Aliases: mahalanofix mahalanofuz
> ### Keywords: multivariate
> 
> ### ** Examples
> 
>   x <- c(1,2,3,4,5,6,7,8,9,10)
>   y <- c(1,2,3,8,7,6,5,8,9,10)
>   mahalanofix(cbind(x,y),gv=c(0,0,0,1,1,1,1,1,0,0))
>   mahalanofix(cbind(x,y),gv=c(0,0,0,1,1,1,1,0,0,0))
>   mahalanofix(cbind(x,y),gv=c(0,0,0,1,1,1,1,1,0,0),method="mcd")
>   mahalanofuz(cbind(x,y),gv=c(0,0,0.5,0.5,1,1,1,0.5,0.5,0))
> 
> 
> 
> cleanEx()
> nameEx("mahalconf")
> ### * mahalconf
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: mahalconf
> ### Title: Mahalanobis fixed point clusters initial configuration
> ### Aliases: mahalconf
> ### Keywords: multivariate cluster
> 
> ### ** Examples
> 
>   set.seed(4634)
>   face <- rFace(600,dMoNo=2,dNoEy=0,p=2)
>   mahalconf(face,no=200,startn=20,covall=cov(face),plot="start")
  [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
 [13] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
 [25] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
 [37] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
 [49] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
 [61] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
 [73] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
 [85] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
 [97] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
[109] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
[121] FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
[133] FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE
[145] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
[157] FALSE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
[169]  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
[181] FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE
[193]  TRUE FALSE FALSE FALSE FALSE  TRUE FALSE  TRUE FALSE FALSE FALSE FALSE
[205] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
[217] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
[229] FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
[241]  TRUE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
[253] FALSE FALSE FALSE FALSE FALSE  TRUE FALSE  TRUE FALSE FALSE FALSE FALSE
[265] FALSE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
[277]  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
[289]  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE
[301] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
[313] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
[325] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
[337] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
[349] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
[361] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
[373] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
[385] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
[397] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
[409] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
[421] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
[433] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
[445] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
[457] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
[469] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
[481] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
[493] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
[505] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
[517] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
[529] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
[541] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
[553] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
[565] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
[577] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
[589] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
> 
> 
> 
> cleanEx()
> nameEx("mergenormals")
> ### * mergenormals
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: mergenormals
> ### Title: Clustering by merging Gaussian mixture components
> ### Aliases: mergenormals summary.mergenorm print.summary.mergenorm
> ### Keywords: multivariate cluster
> 
> ### ** Examples
> 
>   require(mclust)
Loading required package: mclust
Package 'mclust' version 5.4
Type 'citation("mclust")' for citing this R package in publications.
>   require(MASS)
Loading required package: MASS
>   options(digits=3)
>   data(crabs)
>   dc <- crabs[,4:8]
>   cm <- mclustBIC(crabs[,4:8],G=9,modelNames="EEE")
>   scm <- summary(cm,crabs[,4:8])
>   cmnbhat <- mergenormals(crabs[,4:8],scm,method="bhat")
>   summary(cmnbhat)
* Merging Gaussian mixture components *

 Method:  bhat , cutoff value:  0.1 
 Original number of components:  9 
 Number of clusters after merging:  4 
 Values at which clusters were merged: 
     [,1]   [,2]
[1,]    8 0.5746
[2,]    7 0.2503
[3,]    6 0.2307
[4,]    5 0.1443
[5,]    4 0.1258
[6,]    3 0.0999
 Components assigned to clusters: 
      [,1]
 [1,]    1
 [2,]    1
 [3,]    2
 [4,]    3
 [5,]    2
 [6,]    1
 [7,]    4
 [8,]    4
 [9,]    3
>   cmndemp <- mergenormals(crabs[,4:8],scm,method="demp")
>   summary(cmndemp)
* Merging Gaussian mixture components *

 Method:  demp , cutoff value:  0.025 
 Original number of components:  9 
 Number of clusters after merging:  4 
 Values at which clusters were merged: 
     [,1]   [,2]
[1,]    8 0.1902
[2,]    7 0.0714
[3,]    6 0.0694
[4,]    5 0.0351
[5,]    4 0.0266
[6,]    3 0.0220
 Components assigned to clusters: 
      [,1]
 [1,]    1
 [2,]    1
 [3,]    2
 [4,]    3
 [5,]    2
 [6,]    1
 [7,]    4
 [8,]    4
 [9,]    3
> # Other methods take a bit longer, but try them!
> # The values of by and M below are still chosen for reasonably fast execution.
> # cmnrr <- mergenormals(crabs[,4:8],scm,method="ridge.ratio",by=0.05)
> # cmd <- mergenormals(crabs[,4:8],scm,method="dip.tantrum",by=0.05)
> # cmp <- mergenormals(crabs[,4:8],scm,method="predictive",M=3)
> 
> 
> 
> cleanEx()

detaching ‘package:MASS’, ‘package:mclust’

> nameEx("mergeparameters")
> ### * mergeparameters
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: mergeparameters
> ### Title: New parameters from merging two Gaussian mixture components
> ### Aliases: mergeparameters
> ### Keywords: multivariate cluster
> 
> ### ** Examples
> 
>   options(digits=3)
>   set.seed(98765)
>   require(mclust)
Loading required package: mclust
Package 'mclust' version 5.4
Type 'citation("mclust")' for citing this R package in publications.
>   iriss <- iris[sample(150,20),-5]
>   irisBIC <- mclustBIC(iriss)
>   siris <- summary(irisBIC,iriss)
>   probs <- siris$parameters$pro
>   muarray <- siris$parameters$mean
>   Sigmaarray <- siris$parameters$variance$sigma
>   z <- siris$z
>   mpi <- mergeparameters(iriss,1,2,probs,muarray,Sigmaarray,z)
>   mpi$probs
[1] 1.0 0.4
>   mpi$muarray
             [,1]  [,2]
Sepal.Length 5.72 4.963
Sepal.Width  2.95 3.250
Petal.Length 3.47 1.450
Petal.Width  1.07 0.262
> 
> 
> 
> cleanEx()

detaching ‘package:mclust’

> nameEx("minsize")
> ### * minsize
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: minsize
> ### Title: Minimum size of regression fixed point cluster
> ### Aliases: minsize
> ### Keywords: univar cluster
> 
> ### ** Examples
> 
>   minsize(500,4,7000,2)
[1] 127
> 
> 
> 
> cleanEx()
> nameEx("mixdens")
> ### * mixdens
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: mixdens
> ### Title: Density of multivariate Gaussian mixture, mclust
> ###   parameterisation
> ### Aliases: mixdens
> ### Keywords: cluster multivariate
> 
> ### ** Examples
> 
>   set.seed(98765)
>   require(mclust)
Loading required package: mclust
Package 'mclust' version 5.4
Type 'citation("mclust")' for citing this R package in publications.
>   iriss <- iris[sample(150,20),-5]
>   irisBIC <- mclustBIC(iriss)
>   siris <- summary(irisBIC,iriss)
>   round(mixdens(siris$modelName,iriss,siris$parameters),digits=2)
  122    42    45    86    70   137    37    46   134    88    72   123     1 
 0.18  0.70  0.47  0.07  0.25  0.08  1.38 11.26  0.15  0.23  0.24  0.04 10.90 
  147    51   120     9    18    28    54 
 0.06  0.03  0.11  1.68  2.98  5.05  0.32 
> 
> 
> 
> cleanEx()

detaching ‘package:mclust’

> nameEx("mixpredictive")
> ### * mixpredictive
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: mixpredictive
> ### Title: Prediction strength of merged Gaussian mixture
> ### Aliases: mixpredictive
> ### Keywords: cluster multivariate
> 
> ### ** Examples
> 
>   set.seed(98765)
>   iriss <- iris[sample(150,20),-5]
>   mp <- mixpredictive(iriss,2,2,M=2)
> 
> 
> 
> cleanEx()
> nameEx("mvdcoord")
> ### * mvdcoord
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: mvdcoord
> ### Title: Mean/variance differences discriminant coordinates
> ### Aliases: mvdcoord
> ### Keywords: multivariate classif
> 
> ### ** Examples
> 
>   set.seed(4634)
>   face <- rFace(300,dMoNo=2,dNoEy=0,p=3)
>   grface <- as.integer(attr(face,"grouping"))
>   mcf <- mvdcoord(face,grface)
>   plot(mcf$proj,col=grface)
>   # ...done in one step by function plotcluster.
> 
> 
> 
> cleanEx()
> nameEx("ncoord")
> ### * ncoord
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: ncoord
> ### Title: Neighborhood based discriminant coordinates
> ### Aliases: ncoord
> ### Keywords: multivariate classif
> 
> ### ** Examples
> 
>   set.seed(4634)
>   face <- rFace(600,dMoNo=2,dNoEy=0)
>   grface <- as.integer(attr(face,"grouping"))
>   ncf <- ncoord(face,grface)
>   plot(ncf$proj,col=grface)
>   ncf2 <- ncoord(face,grface,weighted=TRUE)
>   plot(ncf2$proj,col=grface)
>   # ...done in one step by function plotcluster.
> 
> 
> 
> cleanEx()
> nameEx("neginc")
> ### * neginc
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: neginc
> ### Title: Neg-entropy normality index for cluster validation
> ### Aliases: neginc
> ### Keywords: cluster
> 
> ### ** Examples
> 
>   options(digits=3)
>   iriss <- as.matrix(iris[c(1:10,51:55,101:105),-5])
>   irisc <- as.numeric(iris[c(1:10,51:55,101:105),5])
>   neginc(iriss,irisc)
[1] -2.92
> 
> 
> 
> cleanEx()
> nameEx("nselectboot")
> ### * nselectboot
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: nselectboot
> ### Title: Selection of the number of clusters via bootstrap
> ### Aliases: nselectboot
> ### Keywords: cluster multivariate
> 
> ### ** Examples
>   
>   set.seed(20000)
>   face <- rFace(50,dMoNo=2,dNoEy=0,p=2)
>   nselectboot(dist(face),B=2,clustermethod=disthclustCBI,
+    method="average",krange=5:7)
>   nselectboot(dist(face),B=2,clustermethod=claraCBI,
+    classification="centroid",krange=5:7)
>   nselectboot(face,B=2,clustermethod=kmeansCBI,
+    classification="centroid",krange=5:7)
> # Of course use larger B in a real application.
> 
> 
> 
> cleanEx()
> nameEx("pamk")
> ### * pamk
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: pamk
> ### Title: Partitioning around medoids with estimation of number of
> ###   clusters
> ### Aliases: pamk
> ### Keywords: cluster multivariate
> 
> ### ** Examples
> 
>   options(digits=3)
>   set.seed(20000)
>   face <- rFace(50,dMoNo=2,dNoEy=0,p=2)
>   pk1 <- pamk(face,krange=1:5,criterion="asw",critout=TRUE)
1  clusters  0 
2  clusters  0.742 
3  clusters  0.748 
4  clusters  0.581 
5  clusters  0.544 
>   pk2 <- pamk(face,krange=1:5,criterion="multiasw",ns=2,critout=TRUE)
1  clusters  0 
2  clusters  0.749 
3  clusters  0.747 
4  clusters  0.563 
5  clusters  0.544 
> # "multiasw" is better for larger data sets, use larger ns then.
>   pk3 <- pamk(face,krange=1:5,criterion="ch",critout=TRUE)
1  clusters  0 
2  clusters  181 
3  clusters  210 
4  clusters  204 
5  clusters  181 
> 
> 
> 
> cleanEx()
> nameEx("piridge")
> ### * piridge
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: piridge
> ### Title: Ridgeline Pi-function
> ### Aliases: piridge
> ### Keywords: cluster multivariate
> 
> ### ** Examples
> 
>   q <- piridge(seq(0,1,0.1),c(1,1),c(2,5),diag(2),diag(2))
> 
> 
> 
> cleanEx()
> nameEx("piridge.zeroes")
> ### * piridge.zeroes
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: piridge.zeroes
> ### Title: Extrema of two-component Gaussian mixture
> ### Aliases: piridge.zeroes
> ### Keywords: cluster multivariate
> 
> ### ** Examples
> 
>   q <- piridge.zeroes(0.2,c(1,1),c(2,5),diag(2),diag(2),by=0.1)
> 
> 
> 
> cleanEx()
> nameEx("plotcluster")
> ### * plotcluster
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: plotcluster
> ### Title: Discriminant projection plot.
> ### Aliases: plotcluster
> ### Keywords: multivariate classif
> 
> ### ** Examples
> 
> set.seed(4634)
> face <- rFace(300,dMoNo=2,dNoEy=0)
> grface <- as.integer(attr(face,"grouping"))
> plotcluster(face,grface)
> plotcluster(face,grface==1)
> plotcluster(face,grface, clnum=1, method="vbc")
[1] "Cluster indicator has more than 2 values"
> 
> 
> 
> cleanEx()
> nameEx("prediction.strength")
> ### * prediction.strength
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: prediction.strength
> ### Title: Prediction strength for estimating number of clusters
> ### Aliases: prediction.strength print.predstr
> ### Keywords: cluster multivariate
> 
> ### ** Examples
> 
>   options(digits=3)
>   set.seed(98765)
>   iriss <- iris[sample(150,20),-5]
>   prediction.strength(iriss,2,3,M=3)
Prediction strength 
Clustering method:  kmeans 
Maximum number of clusters:  3 
Resampled data sets:  3 
Mean pred.str. for numbers of clusters:  1 1 0.733 
Cutoff value:  0.8 
Largest number of clusters better than cutoff:  2 
>   prediction.strength(iriss,2,3,M=3,clustermethod=claraCBI)
Prediction strength 
Clustering method:  clara/pam 
Maximum number of clusters:  3 
Resampled data sets:  3 
Mean pred.str. for numbers of clusters:  1 1 0.45 
Cutoff value:  0.8 
Largest number of clusters better than cutoff:  2 
> # The examples are fast, but of course M should really be larger.
> 
> 
> 
> cleanEx()
> nameEx("rFace")
> ### * rFace
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: rFace
> ### Title: "Face-shaped" clustered benchmark datasets
> ### Aliases: rFace
> ### Keywords: data
> 
> ### ** Examples
> 
>   set.seed(4634)
>   face <- rFace(600,dMoNo=2,dNoEy=0)
>   grface <- as.integer(attr(face,"grouping"))
>   plot(face, col = grface)
> #  pairs(face, col = grface, main ="rFace(600,dMoNo=2,dNoEy=0)")
> 
> 
> 
> cleanEx()
> nameEx("randcmatrix")
> ### * randcmatrix
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: randcmatrix
> ### Title: Random partition matrix
> ### Aliases: randcmatrix
> ### Keywords: cluster
> 
> ### ** Examples
> 
> set.seed(111)
> randcmatrix(10,2,1)
      [,1] [,2]
 [1,]    0    1
 [2,]    0    1
 [3,]    1    0
 [4,]    0    1
 [5,]    1    0
 [6,]    1    0
 [7,]    1    0
 [8,]    0    1
 [9,]    1    0
[10,]    1    0
> 
> 
> 
> cleanEx()
> nameEx("randconf")
> ### * randconf
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: randconf
> ### Title: Generate a sample indicator vector
> ### Aliases: randconf
> ### Keywords: distribution
> 
> ### ** Examples
> 
>   randconf(10,3)
 [1] FALSE FALSE  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE
> 
> 
> 
> cleanEx()
> nameEx("regmix")
> ### * regmix
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: regmix
> ### Title: Mixture Model ML for Clusterwise Linear Regression
> ### Aliases: regmix regem
> ### Keywords: cluster regression
> 
> ### ** Examples
> 
> ## Not run: 
> ##D # This apparently gives slightly different
> ##D # but data-analytically fine results
> ##D # on some versions of R.
> ##D set.seed(12234)
> ##D data(tonedata)
> ##D attach(tonedata)
> ##D rmt1 <- regmix(stretchratio,tuned,nclust=1:2)
> ##D # nclust=1:2 makes the example fast;
> ##D # a more serious application would rather use the default.
> ##D rmt1$g
> ##D round(rmt1$bic,digits=2)
> ##D # start with initial parameter values
> ##D cln <- 3
> ##D n <- 150
> ##D initcoef <- cbind(c(2,0),c(0,1),c(0,2.5))
> ##D initvar <- c(0.001,0.0001,0.5)
> ##D initeps <- c(0.4,0.3,0.3)
> ##D # computation of m from initial parameters
> ##D m <- matrix(nrow=n, ncol=cln)
> ##D stm <- numeric(0)
> ##D for (i in 1:cln)
> ##D   for (j in 1:n){
> ##D     m[j,i] <- initeps[i]*dnorm(tuned[j],mean=initcoef[1,i]+
> ##D               initcoef[2,i]*stretchratio[j], sd=sqrt(initvar[i]))
> ##D   }
> ##D   for (j in 1:n){
> ##D     stm[j] <- sum(m[j,])
> ##D     for (i in 1:cln)
> ##D       m[j,i] <- m[j,i]/stm[j]
> ##D   } 
> ##D rmt2 <- regem(stretchratio, tuned, m, cln)
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("ridgeline")
> ### * ridgeline
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: ridgeline
> ### Title: Ridgeline computation
> ### Aliases: ridgeline
> ### Keywords: cluster multivariate
> 
> ### ** Examples
> 
>   ridgeline(0.5,c(1,1),c(2,5),diag(2),diag(2))
     [,1]
[1,]  1.5
[2,]  3.0
> 
> 
> 
> cleanEx()
> nameEx("ridgeline.diagnosis")
> ### * ridgeline.diagnosis
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: ridgeline.diagnosis
> ### Title: Ridgeline plots, ratios and unimodality
> ### Aliases: ridgeline.diagnosis
> ### Keywords: cluster multivariate
> 
> ### ** Examples
> 
>   muarray <- cbind(c(0,0),c(0,0.1),c(10,10))
>   sigmaarray <- array(c(diag(2),diag(2),diag(2)),dim=c(2,2,3))
>   rd <-
+   ridgeline.diagnosis(c(0.5,0.3,0.2),muarray,sigmaarray,ridgelineplot="matrix",by=0.1)
>   # Much slower but more precise with default by=0.001.
> 
> 
> 
> cleanEx()
> nameEx("simmatrix")
> ### * simmatrix
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: simmatrix
> ### Title: Extracting intersections between clusters from fpc-object
> ### Aliases: simmatrix
> ### Keywords: utilities
> 
> ### ** Examples
> 
> set.seed(190000)
> data(tonedata)
> # Note: If you do not use the installed package, replace this by
> # tonedata <- read.table("(path/)tonedata.txt", header=TRUE)
> attach(tonedata)
> tonefix <- fixreg(stretchratio,tuned,mtf=1,ir=20)
> simmatrix(tonefix)[sseg(2,3)]
[1] 47
> 
> 
> 
> cleanEx()

detaching ‘tonedata’

> nameEx("solvecov")
> ### * solvecov
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: solvecov
> ### Title: Inversion of (possibly singular) symmetric matrices
> ### Aliases: solvecov
> ### Keywords: array
> 
> ### ** Examples
> 
>   x <- c(1,0,0,1,0,1,0,0,1)
>   dim(x) <- c(3,3)
>   solvecov(x)
> 
> 
> 
> cleanEx()
> nameEx("sseg")
> ### * sseg
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: sseg
> ### Title: Position in a similarity vector
> ### Aliases: sseg
> ### Keywords: utilities
> 
> ### ** Examples
> 
> sseg(3,4)
[1] 9
> 
> 
> 
> cleanEx()
> nameEx("tdecomp")
> ### * tdecomp
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: tdecomp
> ### Title: Root of singularity-corrected eigenvalue decomposition
> ### Aliases: tdecomp
> ### Keywords: array
> 
> ### ** Examples
> 
> x <- rnorm(10)
> y <- rnorm(10)
> z <- cov(cbind(x,y))
> round(tdecomp(z),digits=2)
      [,1]  [,2]
[1,] -0.48  1.03
[2,] -0.62 -0.29
> 
> 
> 
> cleanEx()
> nameEx("unimodal.ind")
> ### * unimodal.ind
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: unimodal.ind
> ### Title: Is a fitted denisity unimodal or not?
> ### Aliases: unimodal.ind
> ### Keywords: univar
> 
> ### ** Examples
> 
> unimodal.ind(c(1,3,3,4,2,1,0,0))
[1] TRUE
> 
> 
> 
> cleanEx()
> nameEx("weightplots")
> ### * weightplots
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: weightplots
> ### Title: Ordered posterior plots
> ### Aliases: weightplots
> ### Keywords: multivariate cluster
> 
> ### ** Examples
> 
>   require(mclust)
Loading required package: mclust
Package 'mclust' version 5.4
Type 'citation("mclust")' for citing this R package in publications.
>   require(MASS)
Loading required package: MASS
>   data(crabs)
>   dc <- crabs[,4:8]
>   cm <- mclustBIC(crabs[,4:8],G=9,modelNames="EEE")
>   scm <- summary(cm,crabs[,4:8])
>   weightplots(scm$z,clusternumbers=1:3,ask=FALSE)
>   weightplots(scm$z,clusternumbers=1:3,allcol=1:9, ask=FALSE,
+               legendposition=c(5,0.7))
> # Remove ask=FALSE to have time to watch the plots.
> 
> 
> 
> cleanEx()

detaching ‘package:MASS’, ‘package:mclust’

> nameEx("wfu")
> ### * wfu
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: wfu
> ### Title: Weight function (for Mahalabobis distances)
> ### Aliases: wfu
> ### Keywords: arith
> 
> ### ** Examples
> 
>   md <- seq(0,10,by=0.1)
>   round(wfu(md,ca=5,ca2=8),digits=2)
  [1] 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00
 [16] 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00
 [31] 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00
 [46] 1.00 1.00 1.00 1.00 1.00 1.00 0.97 0.93 0.90 0.87 0.83 0.80 0.77 0.73 0.70
 [61] 0.67 0.63 0.60 0.57 0.53 0.50 0.47 0.43 0.40 0.37 0.33 0.30 0.27 0.23 0.20
 [76] 0.17 0.13 0.10 0.07 0.03 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00
 [91] 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00
> 
> 
> 
> cleanEx()
> nameEx("xtable")
> ### * xtable
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: xtable
> ### Title: Partition crosstable with empty clusters
> ### Aliases: xtable
> ### Keywords: array
> 
> ### ** Examples
> 
>   c1 <- 1:3
>   c2 <- c(1,1,2)
>   xtable(c1,c2,3)
     [,1] [,2] [,3]
[1,]    1    0    0
[2,]    1    0    0
[3,]    0    1    0
> 
> 
> 
> cleanEx()
> nameEx("zmisclassification.matrix")
> ### * zmisclassification.matrix
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: zmisclassification.matrix
> ### Title: Matrix of misclassification probabilities between mixture
> ###   components
> ### Aliases: zmisclassification.matrix
> ### Keywords: cluster multivariate
> 
> ### ** Examples
> 
>   set.seed(12345)
>   m <- rpois(20,lambda=5)
>   dim(m) <- c(5,4)
>   m <- m/apply(m,1,sum)
>   round(zmisclassification.matrix(m,symmetric=FALSE),digits=2) 
     [,1] [,2] [,3] [,4]
[1,] 0.00 0.73 1.00 0.75
[2,] 0.10 0.00 0.56 0.73
[3,] 0.16 0.17 0.00 0.16
[4,] 0.10 0.30 0.56 0.00
> 
> 
> 
> ### * <FOOTER>
> ###
> options(digits = 7L)
> base::cat("Time elapsed: ", proc.time() - base::get("ptime", pos = 'CheckExEnv'),"\n")
Time elapsed:  4.068 0.036 4.111 0 0 
> grDevices::dev.off()
null device 
          1 
> ###
> ### Local variables: ***
> ### mode: outline-minor ***
> ### outline-regexp: "\\(> \\)?### [*]+" ***
> ### End: ***
> quit('no')
